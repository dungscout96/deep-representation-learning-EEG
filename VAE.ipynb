{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow --quiet\n",
        "!pip install webdataset==0.1.62\n",
        "!pip install pytorch-model-summary"
      ],
      "metadata": {
        "id": "NB7Va37UmkNm",
        "outputId": "981870e9-0caa-4f64-a183-eb15a6b020c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NB7Va37UmkNm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: webdataset==0.1.62 in /usr/local/lib/python3.7/dist-packages (0.1.62)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from webdataset==0.1.62) (1.21.6)\n",
            "Requirement already satisfied: braceexpand in /usr/local/lib/python3.7/dist-packages (from webdataset==0.1.62) (0.1.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-model-summary in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-model-summary) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "mathematical-beginning",
      "metadata": {
        "id": "mathematical-beginning"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "import torch\n",
        "import webdataset as wds\n",
        "from itertools import islice\n",
        "import struct, ast\n",
        "from importlib_metadata import version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "version('webdataset')"
      ],
      "metadata": {
        "id": "fMC2c7mLGIIQ",
        "outputId": "49a4c6a0-d373-4f9a-cc5e-be881623d4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "id": "fMC2c7mLGIIQ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.1.62'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "framed-living",
      "metadata": {
        "id": "framed-living",
        "outputId": "027ded48-fd78-4428-8154-722502a89370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "class Logger():\n",
        "    def __init__(self, mode='log'):\n",
        "        self.mode = mode\n",
        "        \n",
        "    def set_model_save_location(self, model_dir):\n",
        "        self.model_dir = f\"saved-model/{model_dir}\"\n",
        "        \n",
        "    def set_experiment(self, experiment_name):\n",
        "        self.experiment_name = experiment_name\n",
        "        log_format = '%(asctime)s %(message)s'\n",
        "        logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
        "                            format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "        fh = logging.FileHandler(os.path.join('training-logs', f'log-{experiment_name}-{datetime.datetime.today()}.txt'))\n",
        "        fh.setFormatter(logging.Formatter(log_format))\n",
        "        logging.getLogger().addHandler(fh)\n",
        "        self.writer = SummaryWriter(f\"runs/{experiment_name}\")\n",
        "            \n",
        "    def log(self, message=\"\"):\n",
        "        if self.mode == 'log':\n",
        "            logging.info(message)\n",
        "        elif self.mode == 'debug':\n",
        "            print(message)\n",
        "\n",
        "    def save_model(self, model, info):\n",
        "        torch.save(model.state_dict(), f\"{self.model_dir}/model-{self.experiment_name}-{info}\")\n",
        "        \n",
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)\n",
        "\n",
        "def add_chan_dim(x):\n",
        "    x = torch.tensor(x)\n",
        "#     x = torch.transpose(x, 0, 1)\n",
        "    return torch.unsqueeze(x,0)\n",
        "\n",
        "\n",
        "def selectLabel(x,lbl):\n",
        "    # # function to select desired label\n",
        "    lbl_idx = [\"id\",\"sex\",\"age\",\"handedness\",\"index\"].index(lbl.lower())\n",
        "    x = x.decode(\"utf-8\").split(\",\")\n",
        "    # return x if lbl_idx == 0 else float(x)\n",
        "    return 1\n",
        "    \n",
        "s3_url = 'https://childmind.s3.us-west-1.amazonaws.com/python/childmind_train.tar' # replace 'train' with 'val' and 'test' accordingly\n",
        "train_data = wds.WebDataset(s3_url).decode().map_dict(npy=add_chan_dim, cls=lambda x: selectLabel(x,'sex')).to_tuple(\"npy\",\"cls\")\n",
        "\n",
        "s3_url = 'https://childmind.s3.us-west-1.amazonaws.com/python/childmind_val.tar' # replace 'train' with 'val' and 'test' accordingly\n",
        "val_data = wds.WebDataset(s3_url).decode().map_dict(npy=add_chan_dim, cls=lambda x: selectLabel(x,'sex')).to_tuple(\"npy\",\"cls\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ignored-eagle",
      "metadata": {
        "id": "ignored-eagle"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module): \n",
        "    \n",
        "    def __init__(self, latent_dim):\n",
        "        # latent_dim: dimension of the latent representation vector\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        encoder_l = [self.encoder_conv_block(True)]\n",
        "        for i in range(2):\n",
        "            encoder_l.append(self.encoder_conv_block())\n",
        "        encoder_l.append(self.encoder_conv_block(False, 32, 32, 3, 1, 0))\n",
        "        encoder_l.append(nn.Flatten())\n",
        "        self.encoder_before_last = nn.ModuleList(encoder_l)\n",
        "        self.encoder_mu = self.encoder_linear_block(960,10)\n",
        "        self.encoder_sigma = self.encoder_linear_block(960, latent_dim)\n",
        "                            \n",
        "        decoder_l = [self.decoder_linear_block(latent_dim, 960)]\n",
        "        decoder_l.append(self.decoder_conv_block(False, 32, 32, 3, 1, 0))\n",
        "        for i in range(2):\n",
        "            decoder_l.append(self.decoder_conv_block())\n",
        "        decoder_l.append(self.decoder_conv_block(True))\n",
        "        self.decoder = nn.ModuleList(decoder_l)\n",
        "    \n",
        "    def encoder_conv_block(self, is_start=False, in_channels=32, out_channels=32, kernel_size=6, stride=2, padding=2):\n",
        "        if is_start:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(1, out_channels, kernel_size, stride, padding),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "    def encoder_linear_block(self, in_chan, out_chan):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_chan, out_chan),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    def decoder_conv_block(self, is_last=False, in_channels=32, out_channels=32, kernel_size=6, stride=2, padding=2):\n",
        "        if is_last:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, 1, kernel_size, stride, padding),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "    \n",
        "    def decoder_linear_block(self, in_chan, out_chan):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_chan, out_chan),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for f in self.encoder_before_last:\n",
        "            x = f(x)\n",
        "\n",
        "        mu = self.encoder_mu(x)\n",
        "        sigma = self.encoder_sigma(x)\n",
        "        z = self.sample_z(mu, sigma)\n",
        "        \n",
        "        x = self.decoder[0](z)\n",
        "        x = x.view(-1, 32, 1, 30) # reshape the tensor to be expected dimension for ConvTranspose\n",
        "        for i in range(1,len(self.decoder)):\n",
        "            f = self.decoder[i]\n",
        "            x = f(x) \n",
        "            \n",
        "        return mu, sigma, x\n",
        "    \n",
        "    def sample_z(self, mu, sigma):\n",
        "        # Input\n",
        "        #     mu:     [batch_size, self.latent_size] the predicted mu value for each sample in the batch\n",
        "        #     sigma:  [batch_size, self.latent_size] the predicted diag elem of sigma value for each sample in the batch\n",
        "        # Output\n",
        "        #     z: [batch_size, self.latent_size] the latent representation of each sample in the batch\n",
        "        # Reference: https://agustinus.kristia.de/techblog/2016/12/10/variational-autoencoder/\n",
        "        \n",
        "        # eps ~ N(0,1)        \n",
        "        batch_size = mu.size()[0]\n",
        "        eps = torch.randn((batch_size,1), device=device, dtype=dtype)\n",
        "        z = mu + sigma/2*eps\n",
        "        \n",
        "        return z\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c39cb173",
      "metadata": {
        "scrolled": false,
        "id": "c39cb173",
        "outputId": "9c32b6ba-8661-4105-d337-7ed86919c4d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-822d0959bde9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_model_summary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_model_summary/model_summary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-fbef8de9034a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_sigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-fbef8de9034a>\u001b[0m in \u001b[0;36msample_z\u001b[0;34m(self, mu, sigma)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ],
      "source": [
        "vae = VAE(10)\n",
        "from pytorch_model_summary import summary\n",
        "print(summary(vae, torch.zeros((1, 1, 24, 256), device=device), show_input=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d6724f9",
      "metadata": {
        "id": "9d6724f9"
      },
      "source": [
        "KL divergence loss\n",
        "![kl_loss](https://github.com/dungscout96/deep-representation-learning-EEG/blob/master/images/kl_loss.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "48aa32b1",
      "metadata": {
        "id": "48aa32b1"
      },
      "outputs": [],
      "source": [
        "def beta_vae_loss(mu_hat, sigma_hat, x_hat, x_target):\n",
        "    likelihood_loss = F.mse_loss(x_hat, x_target)\n",
        "    kl_loss = 0.5 * torch.sum(torch.exp(sigma_hat) + torch.pow(mu_hat,2) - torch.ones((1,mu_hat.size()[1]), device=device, dtype=dtype) - mu_hat, axis=1)\n",
        "    return torch.mean(likelihood_loss + kl_loss) # average the loss of batch\n",
        "\n",
        "def train(model, loader_train, optimizer, loader_val, epochs, logger, device, dtype):\n",
        "    \"\"\" \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    - logger: Logger object for logging purpose\n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    loss_array = []\n",
        "    num_batch = 0\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    print('Begin trainning...')\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            if e == 0:\n",
        "                num_batch += 1\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            mu_hat, sigma_hat, x_hat = model(x)\n",
        "            \n",
        "            loss = beta_vae_loss(mu_hat, sigma_hat, x_hat, x)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_array.append(loss.item())\n",
        "            if t % 100 == 0:\n",
        "                # logger.writer.add_scalar(\"Loss/train\", loss.item(), e*num_batch+t)\n",
        "                # logger.log('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "                print(f'Epoch {e}, Iteration {t}, loss = {loss.item()}')\n",
        "#         train_acc = check_accuracy(loader_train, 'train', model, device, dtype, logger)\n",
        "#         logger.writer.add_scalar(\"Acc/train\", train_acc, e)        \n",
        "        # get validation loss\n",
        "#         model.eval()\n",
        "#         val_loss = check_accuracy(loader_val, 'val', model, device, dtype, logger)\n",
        "#         logger.writer.add_scalar(\"Acc/valid\", val_acc, e)        \n",
        "#         logger.log()\n",
        "        \n",
        "        # Save model per fixed epoch interval\n",
        "        if e > 0 and e % 10 == 0:\n",
        "            logger.save_model(model,f\"epoch{e}\")\n",
        "#         elif val_acc >= 0.83:\n",
        "#             logger.save_model(model,f\"valacc83-epoch{e}\")\n",
        "#         elif val_acc >= 0.84:\n",
        "#             logger.save_model(model,f\"valacc84-epoch{e}\")\n",
        "    # save final model\n",
        "    logger.save_model(model,f\"epoch{e}\")\n",
        "    return model, loss_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2cb70ae0",
      "metadata": {
        "id": "2cb70ae0",
        "outputId": "2cf2f924-1800-4307-a767-8c458094e6f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted mu dim: torch.Size([30, 10])\n",
            "Predicted sigma dim: torch.Size([30, 10])\n",
            "Predicted x dim: torch.Size([30, 1, 24, 256])\n",
            "Loss shape: torch.Size([30])\n"
          ]
        }
      ],
      "source": [
        "mu_pred, sigma_pred, x_pred = vae(torch.zeros(30,1,24,256))\n",
        "\n",
        "print(f'Predicted mu dim: {mu_pred.size()}')\n",
        "print(f'Predicted sigma dim: {sigma_pred.size()}')\n",
        "print(f'Predicted x dim: {x_pred.size()}')\n",
        "\n",
        "loss = beta_vae_loss(mu_pred, sigma_pred, x_pred, torch.ones(30,1,24,256))\n",
        "print(f'Loss shape: {loss.size()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "54ec2618",
      "metadata": {
        "id": "54ec2618",
        "outputId": "400e57b2-add1-4702-ce61-c52ec8b89bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin trainning...\n",
            "Epoch 0, Iteration 0, loss = 1603.4267578125\n",
            "Epoch 0, Iteration 100, loss = 27178.91015625\n",
            "Epoch 0, Iteration 200, loss = 417.28564453125\n",
            "Epoch 0, Iteration 300, loss = 1699.42578125\n",
            "Epoch 0, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 0, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 1, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 1, Iteration 100, loss = 27178.91015625\n",
            "Epoch 1, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 1, Iteration 300, loss = 1699.42578125\n",
            "Epoch 1, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 1, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 2, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 2, Iteration 100, loss = 27178.91015625\n",
            "Epoch 2, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 2, Iteration 300, loss = 1699.42578125\n",
            "Epoch 2, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 2, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 3, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 3, Iteration 100, loss = 27178.91015625\n",
            "Epoch 3, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 3, Iteration 300, loss = 1699.42578125\n",
            "Epoch 3, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 3, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 4, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 4, Iteration 100, loss = 27178.91015625\n",
            "Epoch 4, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 4, Iteration 300, loss = 1699.42578125\n",
            "Epoch 4, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 4, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 5, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 5, Iteration 100, loss = 27178.91015625\n",
            "Epoch 5, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 5, Iteration 300, loss = 1699.42578125\n",
            "Epoch 5, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 5, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 6, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 6, Iteration 100, loss = 27178.91015625\n",
            "Epoch 6, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 6, Iteration 300, loss = 1699.42578125\n",
            "Epoch 6, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 6, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 7, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 7, Iteration 100, loss = 27178.91015625\n",
            "Epoch 7, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 7, Iteration 300, loss = 1699.42578125\n",
            "Epoch 7, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 7, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 8, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 8, Iteration 100, loss = 27178.91015625\n",
            "Epoch 8, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 8, Iteration 300, loss = 1699.42578125\n",
            "Epoch 8, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 8, Iteration 500, loss = 497.4497985839844\n",
            "Epoch 9, Iteration 0, loss = 1603.426513671875\n",
            "Epoch 9, Iteration 100, loss = 27178.91015625\n",
            "Epoch 9, Iteration 200, loss = 417.2856140136719\n",
            "Epoch 9, Iteration 300, loss = 1699.42578125\n",
            "Epoch 9, Iteration 400, loss = 6631.20068359375\n",
            "Epoch 9, Iteration 500, loss = 497.4497985839844\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1a48f456f4a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-e60f69ff080b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader_train, optimizer, loader_val, epochs, logger, device, dtype)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#             logger.save_model(model,f\"valacc84-epoch{e}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# save final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"epoch{e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-85b0e0f11b9a>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, model, info)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{self.model_dir}/model-{self.experiment_name}-{info}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mUSE_GPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Logger' object has no attribute 'experiment_name'"
          ]
        }
      ],
      "source": [
        "logger = Logger()\n",
        "logger.set_model_save_location('VAE')\n",
        "seed = 0\n",
        "experiment = f'VAE-seed{seed}'\n",
        "# logger.set_experiment(experiment)\n",
        "\n",
        "batch_size = 128\n",
        "loader_train = DataLoader(train_data, batch_size=batch_size)\n",
        "loader_val = DataLoader(val_data, batch_size=batch_size)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=0.0001)\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "trained_model, loss_values = train(vae, loader_train, optimizer, loader_val, 10, logger, device, dtype)\n",
        "\n",
        "from google.colab import files\n",
        "np.save('losses.npy',loss_values)\n",
        "torch.save(trained_model.state_dict(),'vae_10_epochs')\n",
        "files.download('vae_10_epochs') \n",
        "files.download('losses.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd5d057",
      "metadata": {
        "id": "5fd5d057"
      },
      "outputs": [],
      "source": [
        "def out_W(W, F, P, S):\n",
        "    return np.floor((W - F + 2*P)/S + 1)\n",
        "\n",
        "W = 256\n",
        "for i in range(3):\n",
        "    W = out_W(W, 6, 2, 2)\n",
        "\n",
        "print(W)\n",
        "print(out_W(W, 3, 0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c5e2bd",
      "metadata": {
        "id": "09c5e2bd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "VAE.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}