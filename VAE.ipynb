{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "NB7Va37UmkNm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NB7Va37UmkNm",
    "outputId": "4ccf8a6d-a603-4c6b-aadd-41374ccdfbd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdataset==0.1.62\n",
      "  Downloading webdataset-0.1.62-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: numpy in /home/dtyoung/.conda/envs/ml/lib/python3.7/site-packages (from webdataset==0.1.62) (1.19.2)\n",
      "Requirement already satisfied: braceexpand in /home/dtyoung/.conda/envs/ml/lib/python3.7/site-packages (from webdataset==0.1.62) (0.1.7)\n",
      "Installing collected packages: webdataset\n",
      "  Attempting uninstall: webdataset\n",
      "    Found existing installation: webdataset 0.1.103\n",
      "    Uninstalling webdataset-0.1.103:\n",
      "      Successfully uninstalled webdataset-0.1.103\n",
      "Successfully installed webdataset-0.1.62\n",
      "Requirement already satisfied: pytorch-model-summary in /home/dtyoung/.conda/envs/ml/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: tqdm in /home/dtyoung/.conda/envs/ml/lib/python3.7/site-packages (from pytorch-model-summary) (4.59.0)\n",
      "Requirement already satisfied: numpy in /home/dtyoung/.conda/envs/ml/lib/python3.7/site-packages (from pytorch-model-summary) (1.19.2)\n",
      "Requirement already satisfied: torch in /home/dtyoung/.conda/envs/ml/lib/python3.7/site-packages (from pytorch-model-summary) (1.8.0)\n",
      "Requirement already satisfied: typing_extensions in /home/dtyoung/.conda/envs/ml/lib/python3.7/site-packages (from torch->pytorch-model-summary) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow --quiet\n",
    "!pip install webdataset==0.1.62\n",
    "!pip install pytorch-model-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Ln1f0NIUdLsL",
   "metadata": {
    "id": "Ln1f0NIUdLsL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# For visualize input\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mathematical-beginning",
   "metadata": {
    "id": "mathematical-beginning"
   },
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "from itertools import islice\n",
    "import struct, ast\n",
    "from importlib_metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fMC2c7mLGIIQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "fMC2c7mLGIIQ",
    "outputId": "c6727503-e1a1-423d-c350-2a8681b6b10e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.62'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('webdataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "egyptian-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_EEG(data, n_sample):\n",
    "    numChan = 24\n",
    "    if len(data[0]) == 2: # torch batch\n",
    "        x_data = data[:][0]\n",
    "    else:\n",
    "        x_data = data\n",
    "    row = col = int(np.sqrt(n_sample))\n",
    "    fig = plt.figure(figsize=(row*10, col*10))\n",
    "    outer = gridspec.GridSpec(row, col)\n",
    "    print(n_sample)\n",
    "    for i in range(n_sample):\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(numChan, 1,\n",
    "                        subplot_spec=outer[i])\n",
    "#             npimg = img[i,:,:,:].numpy()\n",
    "        npimg = x_data[i,:,:,:]\n",
    "        npimg = np.reshape(npimg,(numChan,256))\n",
    "        yax = None\n",
    "        for j in range(numChan):\n",
    "            ax = plt.Subplot(fig, inner[j])\n",
    "            ax.plot(range(256),npimg[j,:],'k')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            fig.add_subplot(ax)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "built-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, role, winLength, numChan, srate, feature, one_channel=False, version=\"\"):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    :param  \n",
    "        path: Filepath to the dataset\n",
    "        role: Role of the dataset. Can be \"train\", \"val\", or \"test\"\n",
    "        winLength: Length of time window. Can be 2 or 15\n",
    "        numChan: Number of channels. Can be 24 or 128\n",
    "        srate: Sampling rate. Supporting 126Hz\n",
    "        feature: Input feature. Can be \"raw\", \"spectral\", or \"topo\"\n",
    "        one_channel: Where input has 1 or 3 channel in depth dimension. Matters when load topo data as number of input channels \n",
    "                are different from original's\n",
    "        version: Any additional information of the datafile. Will be appended to the file name at the end\n",
    "    \"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    x = f[f'X_{role}']\n",
    "    if feature == 'raw':\n",
    "        x = np.transpose(x,(0,2,1))\n",
    "        x = np.reshape(x,(-1,1,numChan,winLength*srate))\n",
    "        min_mat = np.expand_dims(np.min(x,axis=3),3)\n",
    "        print(min_mat.shape)\n",
    "        assert(min_mat[0,0,0] == np.min(x[0,:,0,:]))\n",
    "        assert(min_mat[0,0,1] == np.min(x[0,:,1,:]))\n",
    "        range_mat = np.expand_dims(np.ptp(x,axis=3),axis=3)\n",
    "        print(range_mat.shape)\n",
    "        assert(range_mat[0,0,0] == np.max(x[0,:,0,:])-np.min(x[0,:,0,:]))\n",
    "        assert(range_mat[0,0,1] == np.max(x[0,:,1,:])-np.min(x[0,:,1,:]))\n",
    "        x = (x - min_mat)/range_mat\n",
    "    elif feature == 'topo':\n",
    "        if one_channel:\n",
    "            samples = []\n",
    "            for i in range(x.shape[0]):\n",
    "                image = x[i]\n",
    "                b, g, r = image[0,:, :], image[1,:, :], image[2,:, :]\n",
    "                concat = np.concatenate((b,g,r), axis=1)\n",
    "                samples.append(concat)\n",
    "            x = np.stack(samples)\n",
    "            x = np.reshape(x,(-1,1,x.shape[1],x.shape[2]))\n",
    "    \n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_y_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_y_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    y = f[f'Y_{role}']\n",
    "   \n",
    "    return EEGDataset(x, y, role=='train', role=='val')\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, x, y, train, val):\n",
    "        super(EEGDataset).__init__()\n",
    "#         assert x.shape[0] == y.size\n",
    "        self.x = x\n",
    "        self.y = [y[i][0] for i in range(y.size)]\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "\n",
    "    def __getitem__(self,key):\n",
    "        return (self.x[key], self.y[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "decent-supplement",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'expand_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-821d87e878fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinLength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumChan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_channel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'X_train shape: {len(train_data)}, {train_data[0][0].shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Y_train shape: {len(train_data)}, {train_data[0][1].shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-667e98d2596a>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, role, winLength, numChan, srate, feature, one_channel, version)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumChan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwinLength\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmin_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0;32m--> 215\u001b[0;31m                                      \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'expand_dim'"
     ]
    }
   ],
   "source": [
    "path = './data/'\n",
    "winLength = 2\n",
    "numChan = 24\n",
    "srate = 128\n",
    "feature = 'raw'\n",
    "one_channel = False\n",
    "\n",
    "role = 'train'\n",
    "train_data = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n",
    "print(f'X_train shape: {len(train_data)}, {train_data[0][0].shape}')\n",
    "print(f'Y_train shape: {len(train_data)}, {train_data[0][1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_raw_EEG(train_data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "framed-living",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "framed-living",
    "outputId": "bde5805e-b0f2-48e1-a7ed-80001858185e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "latent dimension: 20\n"
     ]
    }
   ],
   "source": [
    "class Logger():\n",
    "    def __init__(self, mode='log'):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def set_model_save_location(self, model_dir):\n",
    "        self.model_dir = f\"saved-model/{model_dir}\"\n",
    "        \n",
    "    def set_experiment(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        log_format = '%(asctime)s %(message)s'\n",
    "        logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                            format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "        fh = logging.FileHandler(os.path.join('training-logs', f'log-{experiment_name}-{datetime.datetime.today()}.txt'))\n",
    "        fh.setFormatter(logging.Formatter(log_format))\n",
    "        logging.getLogger().addHandler(fh)\n",
    "        self.writer = SummaryWriter(f\"runs/{experiment_name}\")\n",
    "            \n",
    "    def log(self, message=\"\"):\n",
    "        if self.mode == 'log':\n",
    "            logging.info(message)\n",
    "        elif self.mode == 'debug':\n",
    "            print(message)\n",
    "\n",
    "    def save_model(self, model, info):\n",
    "        torch.save(model.state_dict(), f\"{self.model_dir}/model-{self.experiment_name}-{info}\")\n",
    "        \n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "isDebug = False\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)\n",
    "\n",
    "latent_dim = 20\n",
    "print('latent dimension:', str(latent_dim))\n",
    "def add_chan_dim(x):\n",
    "    x = torch.tensor(x)\n",
    "#     x = torch.transpose(x, 0, 1)\n",
    "    return torch.unsqueeze(x,0)\n",
    "\n",
    "\n",
    "def selectLabel(x,lbl):\n",
    "    # # function to select desired label\n",
    "    lbl_idx = [\"id\",\"sex\",\"age\",\"handedness\",\"index\"].index(lbl.lower())\n",
    "    x = x.decode(\"utf-8\").split(\",\")\n",
    "    # return x if lbl_idx == 0 else float(x)\n",
    "    return 1\n",
    "    \n",
    "# s3_url = 'https://childmind.s3.us-west-1.amazonaws.com/python/childmind_train.tar' # replace 'train' with 'val' and 'test' accordingly\n",
    "# train_data = wds.WebDataset(s3_url).decode().map_dict(npy=add_chan_dim, cls=lambda x: selectLabel(x,'sex')).to_tuple(\"npy\",\"cls\")\n",
    "\n",
    "# s3_url = 'https://childmind.s3.us-west-1.amazonaws.com/python/childmind_val.tar' # replace 'train' with 'val' and 'test' accordingly\n",
    "# val_data = wds.WebDataset(s3_url).decode().map_dict(npy=add_chan_dim, cls=lambda x: selectLabel(x,'sex')).to_tuple(\"npy\",\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ignored-eagle",
   "metadata": {
    "id": "ignored-eagle"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module): \n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        # latent_dim: dimension of the latent representation vector\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = 1\n",
    "        self.gamma = 1000.\n",
    "        self.loss_type = 'H'\n",
    "        self.C_max = torch.tensor([25])\n",
    "        self.C_stop_iter = 1e5\n",
    "\n",
    "        encoder_l = [self.encoder_conv_block(has_maxpool=False, in_channels=1, out_channels=32, kernel_size=6, stride=2, padding=0)]\n",
    "        encoder_l.append(self.encoder_conv_block(has_maxpool=False, in_channels=32, out_channels=32, kernel_size=6, stride=2, padding=0))\n",
    "        encoder_l.append(nn.Flatten())\n",
    "        encoder_l.append(self.encoder_linear_block(6144, 1024))\n",
    "        encoder_l.append(self.encoder_linear_block(1024, 1024))        \n",
    "        self.encoder_before_last = nn.ModuleList(encoder_l)\n",
    "        self.encoder_mu = self.encoder_linear_block(1024, latent_dim)\n",
    "        self.encoder_log_var = self.encoder_linear_block(1024, latent_dim)\n",
    "                            \n",
    "        decoder_l = [self.decoder_linear_block(latent_dim, 1024)]\n",
    "        decoder_l.append(self.decoder_linear_block(1024, 1024))\n",
    "        decoder_l.append(self.decoder_linear_block(1024, 6144))\n",
    "        decoder_l.append(self.decoder_conv_block(has_maxpool=True, in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1))\n",
    "        decoder_l.append(self.decoder_conv_block(has_maxpool=False, in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1))\n",
    "        decoder_l.append(self.decoder_conv_block(has_maxpool=False, in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1))\n",
    "        decoder_l.append(self.decoder_conv_block(has_maxpool=True, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1))\n",
    "        decoder_l.append(self.decoder_conv_block(has_maxpool=False, in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1))\n",
    "        decoder_l.append(self.decoder_conv_block(has_maxpool=True, in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1))\n",
    "        decoder_l.append(self.decoder_conv_block(has_maxpool=False, in_channels=16, out_channels=1, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder = nn.ModuleList(decoder_l)\n",
    "    \n",
    "    def encoder_conv_block(self, has_maxpool=False, in_channels=32, out_channels=32, kernel_size=6, stride=2, padding=2):\n",
    "        if has_maxpool:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, return_indices=True)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "    def encoder_linear_block(self, in_chan, out_chan):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_chan, out_chan),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def decoder_conv_block(self, has_maxpool=False, in_channels=32, out_channels=32, kernel_size=6, stride=2, padding=2):\n",
    "        if has_maxpool:\n",
    "            return nn.Sequential(\n",
    "                nn.MaxUnpool2d(kernel_size=2, stride=2, padding=0),\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "    \n",
    "    def decoder_linear_block(self, in_chan, out_chan):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_chan, out_chan),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        maxpool_indices = []\n",
    "        for f in self.encoder_before_last:\n",
    "            if isinstance(f, nn.Flatten):\n",
    "                x = f(x)\n",
    "            else:\n",
    "                for layer in f:\n",
    "                    if isinstance(layer, nn.MaxPool2d):\n",
    "                        x, indices = layer(x)\n",
    "                        maxpool_indices.append(indices)\n",
    "                    else:\n",
    "                        x = layer(x)\n",
    "            print(x.shape)\n",
    "                \n",
    "        mu = self.encoder_mu(x)\n",
    "        log_var = self.encoder_log_var(x)\n",
    "\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "#         x = self.decoder[0](z)\n",
    "#         x = x.view(-1, 32, 1, 30) # reshape the tensor to be expected dimension for ConvTranspose\n",
    "#         for i in range(1,len(self.decoder)):\n",
    "#             f = self.decoder[i]\n",
    "#             x = f(x) \n",
    "        x = self.decode(z, maxpool_indices)\n",
    "            \n",
    "        return x, mu, log_var\n",
    "    \n",
    "    def decode(self, z, maxpool_indices):\n",
    "        x = self.decoder[0](z) # linear(latent_dim, 1024)\n",
    "        x = self.decoder[1](x) # linear(1024, 1024) \n",
    "        x = self.decoder[2](x) # linear(1024, 6144)\n",
    "        x = x.view(-1, 64, 3, 32) # reshape the tensor to be expected dimension for ConvTranspose\n",
    "        unpool_idx = len(maxpool_indices)-1\n",
    "        for i in range(3,len(self.decoder)):\n",
    "            f = self.decoder[i]\n",
    "            for layer in f:\n",
    "                if isinstance(layer, nn.MaxUnpool2d):\n",
    "                    x = layer(x, maxpool_indices[unpool_idx])\n",
    "                    unpool_idx = unpool_idx-1\n",
    "                else:\n",
    "                    x = layer(x) \n",
    "            print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def generate_sample(self, n_sample):\n",
    "        z = torch.randn((n_sample, self.latent_dim))\n",
    "        print(z)\n",
    "        return self.decode(z)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    "\n",
    "    def sample_z(self, mu, sigma):\n",
    "        # Input\n",
    "        #     mu:     [batch_size, self.latent_size] the predicted mu value for each sample in the batch\n",
    "        #     sigma:  [batch_size, self.latent_size] the predicted diag elem of sigma value for each sample in the batch\n",
    "        # Output\n",
    "        #     z: [batch_size, self.latent_size] the latent representation of each sample in the batch\n",
    "        # Reference: https://agustinus.kristia.de/techblog/2016/12/10/variational-autoencoder/\n",
    "        \n",
    "        # eps ~ N(0,1)        \n",
    "        batch_size = mu.size()[0]\n",
    "        eps = torch.randn((batch_size,1), device=device, dtype=dtype)\n",
    "        z = mu + sigma/2*eps\n",
    "        z = z.to(device=device)\n",
    "        return z\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        kld_weight = kwargs['M_N']  # Account for the minibatch samples from the dataset\n",
    "\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl\n",
    "            loss = recons_loss + self.beta * kld_weight * kld_loss\n",
    "        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf\n",
    "            self.C_max = self.C_max.to(input.device)\n",
    "            C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n",
    "            loss = recons_loss + self.gamma * kld_weight* (kld_loss - C).abs()\n",
    "        else:\n",
    "            raise ValueError('Undefined loss type.')\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c39cb173",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c39cb173",
    "outputId": "e4f64788-2b81-4cbb-ff8d-174f96633bd0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder_before_last): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(6, 6), stride=(2, 2))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(6, 6), stride=(2, 2))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=6144, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (encoder_mu): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (encoder_log_var): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=6144, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "      (1): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "      (1): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "      (1): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 32, 126, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (126 x 1). Kernel size: (6 x 6). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-f278b176d5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_model_summary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# print(summary(vae, sample, show_input=False))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/pytorch_model_summary/model_summary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-a7059e75d503>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m                         \u001b[0mmaxpool_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (126 x 1). Kernel size: (6 x 6). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "class ToCorrectSizeTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        correct_size_sample = torch.zeros(1,24,256)\n",
    "        correct_size_sample[0,:,0:28] = sample[0,2:26,:]\n",
    "        return correct_size_sample\n",
    "\n",
    "vae = VAE(latent_dim)\n",
    "vae = vae.to(device=device)\n",
    "\n",
    "if isDebug:\n",
    "    mnist_train = torchvision.datasets.MNIST('./mnist', train=True, download=True, transform = transforms.Compose([transforms.ToTensor(),ToCorrectSizeTensor()]))\n",
    "    mnist_test = torchvision.datasets.MNIST('./mnist', train=False, download=True, transform = transforms.Compose([transforms.ToTensor(), ToCorrectSizeTensor()]))\n",
    "    sample = torch.unsqueeze(mnist_train[0][0],0)\n",
    "    sample = sample.to(device=device)\n",
    "else:\n",
    "    sample = torch.zeros((1, 1, 256, 6), device=device)\n",
    "print(vae)                         \n",
    "from pytorch_model_summary import summary\n",
    "# print(summary(vae, sample, show_input=False)) \n",
    "print(summary(vae, sample, show_input=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6724f9",
   "metadata": {
    "id": "9d6724f9"
   },
   "source": [
    "KL divergence loss\n",
    "![kl_loss](https://github.com/dungscout96/deep-representation-learning-EEG/blob/master/images/kl_loss.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48aa32b1",
   "metadata": {
    "id": "48aa32b1"
   },
   "outputs": [],
   "source": [
    "# def beta_vae_loss(mu_hat, sigma_hat, x_hat, x_target):\n",
    "#     likelihood_loss = F.mse_loss(x_hat, x_target)\n",
    "#     kl_loss = 0.5 * torch.sum(torch.exp(sigma_hat) + torch.pow(mu_hat,2) - torch.ones((1,mu_hat.size()[1]), device=device, dtype=dtype) - mu_hat, axis=1)\n",
    "#     return torch.mean(likelihood_loss + kl_loss) # average the loss of batch\n",
    "\n",
    "def train(model, loader_train, optimizer, loader_val, epochs, logger, device, dtype, M_N):\n",
    "    \"\"\" \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    - logger: Logger object for logging purpose\n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    loss_array = []\n",
    "    num_batch = 0\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    print('Begin trainning...')\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            if e == 0:\n",
    "                num_batch += 1\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            # mu_hat, sigma_hat, x_hat = model(x)\n",
    "            \n",
    "            # loss = beta_vae_loss(mu_hat, sigma_hat, x_hat, x)\n",
    "            \n",
    "            x_hat, mu_hat, sigma_hat = model(x)\n",
    "            \n",
    "            loss_dict = model.loss_function(x_hat, x, mu_hat, sigma_hat,M_N=M_N)\n",
    "            loss = loss_dict[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_array.append(loss.item())\n",
    "            if t % 100 == 0:\n",
    "                # logger.writer.add_scalar(\"Loss/train\", loss.item(), e*num_batch+t)\n",
    "                # logger.log('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                print(f'Epoch {e}, Iteration {t}, loss = {loss.item()}')\n",
    "#         train_acc = check_accuracy(loader_train, 'train', model, device, dtype, logger)\n",
    "#         logger.writer.add_scalar(\"Acc/train\", train_acc, e)        \n",
    "        # get validation loss\n",
    "#         model.eval()\n",
    "#         val_loss = check_accuracy(loader_val, 'val', model, device, dtype, logger)\n",
    "#         logger.writer.add_scalar(\"Acc/valid\", val_acc, e)        \n",
    "#         logger.log()\n",
    "        \n",
    "        # Save model per fixed epoch interval\n",
    "        # if e > 0 and e % 10 == 0:\n",
    "        #     logger.save_model(model,f\"epoch{e}\")\n",
    "#         elif val_acc >= 0.83:\n",
    "#             logger.save_model(model,f\"valacc83-epoch{e}\")\n",
    "#         elif val_acc >= 0.84:\n",
    "#             logger.save_model(model,f\"valacc84-epoch{e}\")\n",
    "    # save final model\n",
    "    # logger.save_model(model,f\"epoch{e}\")\n",
    "    return model, loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54ec2618",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "54ec2618",
    "outputId": "c197bb05-d769-4c4f-a52b-5e803ecb9c04",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num epochs: 100\n",
      "N/M: 139.416015625\n",
      "Begin trainning...\n",
      "Epoch 0, Iteration 0, loss = 28449.712890625\n",
      "Epoch 0, Iteration 100, loss = 4369.62841796875\n",
      "Epoch 1, Iteration 0, loss = 11643.9443359375\n",
      "Epoch 1, Iteration 100, loss = 146834.765625\n",
      "Epoch 2, Iteration 0, loss = 125107.953125\n",
      "Epoch 2, Iteration 100, loss = 5862.396484375\n",
      "Epoch 3, Iteration 0, loss = 67545.84375\n",
      "Epoch 3, Iteration 100, loss = 98030.484375\n",
      "Epoch 4, Iteration 0, loss = 54954.8359375\n",
      "Epoch 4, Iteration 100, loss = 19488.56640625\n",
      "Epoch 5, Iteration 0, loss = 30581.56640625\n",
      "Epoch 5, Iteration 100, loss = 22413.9140625\n",
      "Epoch 6, Iteration 0, loss = 301351.15625\n",
      "Epoch 6, Iteration 100, loss = 2098550.0\n",
      "Epoch 7, Iteration 0, loss = 49240.41796875\n",
      "Epoch 7, Iteration 100, loss = 16561.283203125\n",
      "Epoch 8, Iteration 0, loss = 145692.703125\n",
      "Epoch 8, Iteration 100, loss = 14073.458984375\n",
      "Epoch 9, Iteration 0, loss = 5427584.5\n",
      "Epoch 9, Iteration 100, loss = 42534.9140625\n",
      "Epoch 10, Iteration 0, loss = 24322.99609375\n",
      "Epoch 10, Iteration 100, loss = 162505.78125\n",
      "Epoch 11, Iteration 0, loss = 20185.55078125\n",
      "Epoch 11, Iteration 100, loss = 65037.09765625\n",
      "Epoch 12, Iteration 0, loss = 54472.8046875\n",
      "Epoch 12, Iteration 100, loss = 98855.078125\n",
      "Epoch 13, Iteration 0, loss = 12984.2646484375\n",
      "Epoch 13, Iteration 100, loss = 13850.6669921875\n",
      "Epoch 14, Iteration 0, loss = 6070444.5\n",
      "Epoch 14, Iteration 100, loss = 200548.375\n",
      "Epoch 15, Iteration 0, loss = 10606.1640625\n",
      "Epoch 15, Iteration 100, loss = 277634.625\n",
      "Epoch 16, Iteration 0, loss = 11566.732421875\n",
      "Epoch 16, Iteration 100, loss = 11790.310546875\n",
      "Epoch 17, Iteration 0, loss = 534077.1875\n",
      "Epoch 17, Iteration 100, loss = 29108.138671875\n",
      "Epoch 18, Iteration 0, loss = 123577.1484375\n",
      "Epoch 18, Iteration 100, loss = 48380.70703125\n",
      "Epoch 19, Iteration 0, loss = 107033.796875\n",
      "Epoch 19, Iteration 100, loss = 249934.046875\n",
      "Epoch 20, Iteration 0, loss = 7579.359375\n",
      "Epoch 20, Iteration 100, loss = 65953.9296875\n",
      "Epoch 21, Iteration 0, loss = 194608.3125\n",
      "Epoch 21, Iteration 100, loss = 85649.859375\n",
      "Epoch 22, Iteration 0, loss = 59023.4453125\n",
      "Epoch 22, Iteration 100, loss = 14192.634765625\n",
      "Epoch 23, Iteration 0, loss = 73346.546875\n",
      "Epoch 23, Iteration 100, loss = 592449.4375\n",
      "Epoch 24, Iteration 0, loss = 25513.884765625\n",
      "Epoch 24, Iteration 100, loss = 4233.0595703125\n",
      "Epoch 25, Iteration 0, loss = 24053.576171875\n",
      "Epoch 25, Iteration 100, loss = 152947.53125\n",
      "Epoch 26, Iteration 0, loss = 26013.828125\n",
      "Epoch 26, Iteration 100, loss = 173411.6875\n",
      "Epoch 27, Iteration 0, loss = 72157.0234375\n",
      "Epoch 27, Iteration 100, loss = 12118.6015625\n",
      "Epoch 28, Iteration 0, loss = 371196.75\n",
      "Epoch 28, Iteration 100, loss = 38260.5078125\n",
      "Epoch 29, Iteration 0, loss = 315187.65625\n",
      "Epoch 29, Iteration 100, loss = 38944.54296875\n",
      "Epoch 30, Iteration 0, loss = 6903.3486328125\n",
      "Epoch 30, Iteration 100, loss = 596893.8125\n",
      "Epoch 31, Iteration 0, loss = 114607.0390625\n",
      "Epoch 31, Iteration 100, loss = 185067.78125\n",
      "Epoch 32, Iteration 0, loss = 2472016.0\n",
      "Epoch 32, Iteration 100, loss = 155467.46875\n",
      "Epoch 33, Iteration 0, loss = 57169.49609375\n",
      "Epoch 33, Iteration 100, loss = 13939.4140625\n",
      "Epoch 34, Iteration 0, loss = 203335.90625\n",
      "Epoch 34, Iteration 100, loss = 149292.828125\n",
      "Epoch 35, Iteration 0, loss = 47407.7890625\n",
      "Epoch 35, Iteration 100, loss = 18990.3046875\n",
      "Epoch 36, Iteration 0, loss = 25781.36328125\n",
      "Epoch 36, Iteration 100, loss = 89867.9921875\n",
      "Epoch 37, Iteration 0, loss = 10447.390625\n",
      "Epoch 37, Iteration 100, loss = 22511.9765625\n",
      "Epoch 38, Iteration 0, loss = 30500.0546875\n",
      "Epoch 38, Iteration 100, loss = 40141.1328125\n",
      "Epoch 39, Iteration 0, loss = 555383.0\n",
      "Epoch 39, Iteration 100, loss = 17636.89453125\n",
      "Epoch 40, Iteration 0, loss = 1040953.4375\n",
      "Epoch 40, Iteration 100, loss = 198456.59375\n",
      "Epoch 41, Iteration 0, loss = 53293.76953125\n",
      "Epoch 41, Iteration 100, loss = 23472.193359375\n",
      "Epoch 42, Iteration 0, loss = 90730.4453125\n",
      "Epoch 42, Iteration 100, loss = 60393.984375\n",
      "Epoch 43, Iteration 0, loss = 6726.08154296875\n",
      "Epoch 43, Iteration 100, loss = 16400.796875\n",
      "Epoch 44, Iteration 0, loss = 28734.38671875\n",
      "Epoch 44, Iteration 100, loss = 4996766.5\n",
      "Epoch 45, Iteration 0, loss = 24017.220703125\n",
      "Epoch 45, Iteration 100, loss = 197600.75\n",
      "Epoch 46, Iteration 0, loss = 30637.75\n",
      "Epoch 46, Iteration 100, loss = 21644.1015625\n",
      "Epoch 47, Iteration 0, loss = 625107.875\n",
      "Epoch 47, Iteration 100, loss = 650200.5\n",
      "Epoch 48, Iteration 0, loss = 9093.9736328125\n",
      "Epoch 48, Iteration 100, loss = 192020.890625\n",
      "Epoch 49, Iteration 0, loss = 169800.734375\n",
      "Epoch 49, Iteration 100, loss = 190730.859375\n",
      "Epoch 50, Iteration 0, loss = 25816.921875\n",
      "Epoch 50, Iteration 100, loss = 41899.37890625\n",
      "Epoch 51, Iteration 0, loss = 14153.849609375\n",
      "Epoch 51, Iteration 100, loss = 38446.54296875\n",
      "Epoch 52, Iteration 0, loss = 25253.83984375\n",
      "Epoch 52, Iteration 100, loss = 383403.21875\n",
      "Epoch 53, Iteration 0, loss = 24180.728515625\n",
      "Epoch 53, Iteration 100, loss = 333416.1875\n",
      "Epoch 54, Iteration 0, loss = 12558.0205078125\n",
      "Epoch 54, Iteration 100, loss = 20574.21875\n",
      "Epoch 55, Iteration 0, loss = 224005.921875\n",
      "Epoch 55, Iteration 100, loss = 169813.84375\n",
      "Epoch 56, Iteration 0, loss = 72795.1953125\n",
      "Epoch 56, Iteration 100, loss = 6661.56591796875\n",
      "Epoch 57, Iteration 0, loss = 609869.6875\n",
      "Epoch 57, Iteration 100, loss = 1250440.25\n",
      "Epoch 58, Iteration 0, loss = 27136.13671875\n",
      "Epoch 58, Iteration 100, loss = 19288.16015625\n",
      "Epoch 59, Iteration 0, loss = 36464.796875\n",
      "Epoch 59, Iteration 100, loss = 175078.484375\n",
      "Epoch 60, Iteration 0, loss = 781067.6875\n",
      "Epoch 60, Iteration 100, loss = 38988.91015625\n",
      "Epoch 61, Iteration 0, loss = 17316.40234375\n",
      "Epoch 61, Iteration 100, loss = 69308.4765625\n",
      "Epoch 62, Iteration 0, loss = 140673.03125\n",
      "Epoch 62, Iteration 100, loss = 592451.9375\n",
      "Epoch 63, Iteration 0, loss = 43082.34765625\n",
      "Epoch 63, Iteration 100, loss = 37690.234375\n",
      "Epoch 64, Iteration 0, loss = 35371.3125\n",
      "Epoch 64, Iteration 100, loss = 216415.984375\n",
      "Epoch 65, Iteration 0, loss = 28033.9453125\n",
      "Epoch 65, Iteration 100, loss = 19841.587890625\n",
      "Epoch 66, Iteration 0, loss = 490851.03125\n",
      "Epoch 66, Iteration 100, loss = 386043.53125\n",
      "Epoch 67, Iteration 0, loss = 2317097.25\n",
      "Epoch 67, Iteration 100, loss = 7229.80908203125\n",
      "Epoch 68, Iteration 0, loss = 178565.0625\n",
      "Epoch 68, Iteration 100, loss = 30478.322265625\n",
      "Epoch 69, Iteration 0, loss = 82604.421875\n",
      "Epoch 69, Iteration 100, loss = 13416.826171875\n",
      "Epoch 70, Iteration 0, loss = 38014.5078125\n",
      "Epoch 70, Iteration 100, loss = 52730.890625\n",
      "Epoch 71, Iteration 0, loss = 104200.9140625\n",
      "Epoch 71, Iteration 100, loss = 94728.09375\n",
      "Epoch 72, Iteration 0, loss = 59425.93359375\n",
      "Epoch 72, Iteration 100, loss = 31034.90625\n",
      "Epoch 73, Iteration 0, loss = 8782.0947265625\n",
      "Epoch 73, Iteration 100, loss = 9315.6953125\n",
      "Epoch 74, Iteration 0, loss = 82645.203125\n",
      "Epoch 74, Iteration 100, loss = 5534826.0\n",
      "Epoch 75, Iteration 0, loss = 72087.125\n",
      "Epoch 75, Iteration 100, loss = 36582.77734375\n",
      "Epoch 76, Iteration 0, loss = 13357.0361328125\n",
      "Epoch 76, Iteration 100, loss = 7564354.5\n",
      "Epoch 77, Iteration 0, loss = 192324.359375\n",
      "Epoch 77, Iteration 100, loss = 47041.52734375\n",
      "Epoch 78, Iteration 0, loss = 23432.287109375\n",
      "Epoch 78, Iteration 100, loss = 21517.421875\n",
      "Epoch 79, Iteration 0, loss = 23919.310546875\n",
      "Epoch 79, Iteration 100, loss = 50535.4375\n",
      "Epoch 80, Iteration 0, loss = 85467.2421875\n",
      "Epoch 80, Iteration 100, loss = 10414.1220703125\n",
      "Epoch 81, Iteration 0, loss = 23056.958984375\n",
      "Epoch 81, Iteration 100, loss = 98340.265625\n",
      "Epoch 82, Iteration 0, loss = 66988.359375\n",
      "Epoch 82, Iteration 100, loss = 76481.6953125\n",
      "Epoch 83, Iteration 0, loss = 26864.431640625\n",
      "Epoch 83, Iteration 100, loss = 300078.53125\n",
      "Epoch 84, Iteration 0, loss = 215843.984375\n",
      "Epoch 84, Iteration 100, loss = 83225.2578125\n",
      "Epoch 85, Iteration 0, loss = 103378.1484375\n",
      "Epoch 85, Iteration 100, loss = 40008.63671875\n",
      "Epoch 86, Iteration 0, loss = 12899.830078125\n",
      "Epoch 86, Iteration 100, loss = 15462.4423828125\n",
      "Epoch 87, Iteration 0, loss = 22223.416015625\n",
      "Epoch 87, Iteration 100, loss = 32785.09375\n",
      "Epoch 88, Iteration 0, loss = 93260.7265625\n",
      "Epoch 88, Iteration 100, loss = 32267.19921875\n",
      "Epoch 89, Iteration 0, loss = 50090.890625\n",
      "Epoch 89, Iteration 100, loss = 26832.888671875\n",
      "Epoch 90, Iteration 0, loss = 163438.171875\n",
      "Epoch 90, Iteration 100, loss = 20324.822265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, Iteration 0, loss = 226498.09375\n",
      "Epoch 91, Iteration 100, loss = 96759.671875\n",
      "Epoch 92, Iteration 0, loss = 588186.375\n",
      "Epoch 92, Iteration 100, loss = 321736.3125\n",
      "Epoch 93, Iteration 0, loss = 438495.96875\n",
      "Epoch 93, Iteration 100, loss = 342720.21875\n",
      "Epoch 94, Iteration 0, loss = 7555736.0\n",
      "Epoch 94, Iteration 100, loss = 30471.552734375\n",
      "Epoch 95, Iteration 0, loss = 24255.861328125\n",
      "Epoch 95, Iteration 100, loss = 8459.2958984375\n",
      "Epoch 96, Iteration 0, loss = 25277.3828125\n",
      "Epoch 96, Iteration 100, loss = 5090.74755859375\n",
      "Epoch 97, Iteration 0, loss = 26999.73046875\n",
      "Epoch 97, Iteration 100, loss = 362719.84375\n",
      "Epoch 98, Iteration 0, loss = 29893.73046875\n",
      "Epoch 98, Iteration 100, loss = 185333.4375\n",
      "Epoch 99, Iteration 0, loss = 20918.1796875\n",
      "Epoch 99, Iteration 100, loss = 64772.40234375\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(latent_dim)\n",
    "logger = Logger()\n",
    "logger.set_model_save_location('VAE')\n",
    "seed = 0\n",
    "experiment = f'VAE-seed{seed}'\n",
    "# logger.set_experiment(experiment)\n",
    "\n",
    "if isDebug:\n",
    "    batch_size = 128\n",
    "    loader_train = DataLoader(mnist_train, batch_size=batch_size)\n",
    "    loader_val = DataLoader(mnist_test, batch_size=batch_size)\n",
    "    M_N = len(mnist_train)/batch_size # for VAE\n",
    "    epochs = 10\n",
    "else:\n",
    "    batch_size = 512\n",
    "    loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    loader_val = DataLoader(val_data, batch_size=batch_size)\n",
    "    M_N = len(train_data)/batch_size # for VAE\n",
    "    epochs = 100\n",
    "\n",
    "print('num epochs:', str(epochs))\n",
    "print('N/M:', str(M_N))\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.00001)\n",
    "\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "trained_model, loss_values = train(trained_model, loader_train, optimizer, loader_val, epochs, logger, device, dtype, M_N)\n",
    "\n",
    "# from google.colab import files\n",
    "# np.save('losses.npy',loss_values)\n",
    "# torch.save(trained_model.state_dict(),'vae_10_epochs')\n",
    "# files.download('vae_10_epochs') \n",
    "# files.download('losses.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09c5e2bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "09c5e2bd",
    "outputId": "d7724e26-8cfb-446b-d6c5-9bb309344683"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15549253d810>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_values[500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "rRxXAQF60PNB",
   "metadata": {
    "id": "rRxXAQF60PNB"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"encoder_before_last.4.0.weight\", \"encoder_before_last.4.0.bias\", \"encoder_before_last.5.0.weight\", \"encoder_before_last.5.0.bias\", \"encoder_before_last.6.0.weight\", \"encoder_before_last.6.0.bias\", \"encoder_before_last.8.0.weight\", \"encoder_before_last.8.0.bias\", \"encoder_before_last.9.0.weight\", \"encoder_before_last.9.0.bias\", \"decoder.5.0.weight\", \"decoder.5.0.bias\", \"decoder.6.1.weight\", \"decoder.6.1.bias\", \"decoder.7.0.weight\", \"decoder.7.0.bias\", \"decoder.8.1.weight\", \"decoder.8.1.bias\", \"decoder.9.0.weight\", \"decoder.9.0.bias\". \n\tUnexpected key(s) in state_dict: \"encoder_before_last.0.1.weight\", \"encoder_before_last.0.1.bias\", \"encoder_before_last.0.1.running_mean\", \"encoder_before_last.0.1.running_var\", \"encoder_before_last.0.1.num_batches_tracked\", \"encoder_before_last.1.1.weight\", \"encoder_before_last.1.1.bias\", \"encoder_before_last.1.1.running_mean\", \"encoder_before_last.1.1.running_var\", \"encoder_before_last.1.1.num_batches_tracked\", \"encoder_before_last.2.1.weight\", \"encoder_before_last.2.1.bias\", \"encoder_before_last.2.1.running_mean\", \"encoder_before_last.2.1.running_var\", \"encoder_before_last.2.1.num_batches_tracked\", \"encoder_before_last.3.1.weight\", \"encoder_before_last.3.1.bias\", \"encoder_before_last.3.1.running_mean\", \"encoder_before_last.3.1.running_var\", \"encoder_before_last.3.1.num_batches_tracked\", \"decoder.1.1.weight\", \"decoder.1.1.bias\", \"decoder.1.1.running_mean\", \"decoder.1.1.running_var\", \"decoder.1.1.num_batches_tracked\", \"decoder.2.1.weight\", \"decoder.2.1.bias\", \"decoder.2.1.running_mean\", \"decoder.2.1.running_var\", \"decoder.2.1.num_batches_tracked\", \"decoder.3.0.weight\", \"decoder.3.0.bias\", \"decoder.3.1.running_mean\", \"decoder.3.1.running_var\", \"decoder.3.1.num_batches_tracked\", \"decoder.4.1.weight\", \"decoder.4.1.bias\", \"decoder.4.1.running_mean\", \"decoder.4.1.running_var\", \"decoder.4.1.num_batches_tracked\". \n\tsize mismatch for encoder_before_last.0.0.weight: copying a param with shape torch.Size([32, 1, 6, 6]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3]).\n\tsize mismatch for encoder_before_last.0.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for encoder_before_last.1.0.weight: copying a param with shape torch.Size([32, 32, 6, 6]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for encoder_before_last.1.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for encoder_before_last.2.0.weight: copying a param with shape torch.Size([32, 32, 6, 6]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for encoder_mu.0.weight: copying a param with shape torch.Size([20, 960]) from checkpoint, the shape in current model is torch.Size([20, 1024]).\n\tsize mismatch for encoder_log_var.0.weight: copying a param with shape torch.Size([20, 960]) from checkpoint, the shape in current model is torch.Size([20, 1024]).\n\tsize mismatch for decoder.0.0.weight: copying a param with shape torch.Size([960, 20]) from checkpoint, the shape in current model is torch.Size([1024, 20]).\n\tsize mismatch for decoder.0.0.bias: copying a param with shape torch.Size([960]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.1.0.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.1.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.2.0.weight: copying a param with shape torch.Size([32, 32, 6, 6]) from checkpoint, the shape in current model is torch.Size([6144, 1024]).\n\tsize mismatch for decoder.2.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([6144]).\n\tsize mismatch for decoder.3.1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for decoder.3.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for decoder.4.0.weight: copying a param with shape torch.Size([32, 1, 6, 6]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for decoder.4.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-5bee7fc5133d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_vae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1224\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"encoder_before_last.4.0.weight\", \"encoder_before_last.4.0.bias\", \"encoder_before_last.5.0.weight\", \"encoder_before_last.5.0.bias\", \"encoder_before_last.6.0.weight\", \"encoder_before_last.6.0.bias\", \"encoder_before_last.8.0.weight\", \"encoder_before_last.8.0.bias\", \"encoder_before_last.9.0.weight\", \"encoder_before_last.9.0.bias\", \"decoder.5.0.weight\", \"decoder.5.0.bias\", \"decoder.6.1.weight\", \"decoder.6.1.bias\", \"decoder.7.0.weight\", \"decoder.7.0.bias\", \"decoder.8.1.weight\", \"decoder.8.1.bias\", \"decoder.9.0.weight\", \"decoder.9.0.bias\". \n\tUnexpected key(s) in state_dict: \"encoder_before_last.0.1.weight\", \"encoder_before_last.0.1.bias\", \"encoder_before_last.0.1.running_mean\", \"encoder_before_last.0.1.running_var\", \"encoder_before_last.0.1.num_batches_tracked\", \"encoder_before_last.1.1.weight\", \"encoder_before_last.1.1.bias\", \"encoder_before_last.1.1.running_mean\", \"encoder_before_last.1.1.running_var\", \"encoder_before_last.1.1.num_batches_tracked\", \"encoder_before_last.2.1.weight\", \"encoder_before_last.2.1.bias\", \"encoder_before_last.2.1.running_mean\", \"encoder_before_last.2.1.running_var\", \"encoder_before_last.2.1.num_batches_tracked\", \"encoder_before_last.3.1.weight\", \"encoder_before_last.3.1.bias\", \"encoder_before_last.3.1.running_mean\", \"encoder_before_last.3.1.running_var\", \"encoder_before_last.3.1.num_batches_tracked\", \"decoder.1.1.weight\", \"decoder.1.1.bias\", \"decoder.1.1.running_mean\", \"decoder.1.1.running_var\", \"decoder.1.1.num_batches_tracked\", \"decoder.2.1.weight\", \"decoder.2.1.bias\", \"decoder.2.1.running_mean\", \"decoder.2.1.running_var\", \"decoder.2.1.num_batches_tracked\", \"decoder.3.0.weight\", \"decoder.3.0.bias\", \"decoder.3.1.running_mean\", \"decoder.3.1.running_var\", \"decoder.3.1.num_batches_tracked\", \"decoder.4.1.weight\", \"decoder.4.1.bias\", \"decoder.4.1.running_mean\", \"decoder.4.1.running_var\", \"decoder.4.1.num_batches_tracked\". \n\tsize mismatch for encoder_before_last.0.0.weight: copying a param with shape torch.Size([32, 1, 6, 6]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3]).\n\tsize mismatch for encoder_before_last.0.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for encoder_before_last.1.0.weight: copying a param with shape torch.Size([32, 32, 6, 6]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for encoder_before_last.1.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for encoder_before_last.2.0.weight: copying a param with shape torch.Size([32, 32, 6, 6]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for encoder_mu.0.weight: copying a param with shape torch.Size([20, 960]) from checkpoint, the shape in current model is torch.Size([20, 1024]).\n\tsize mismatch for encoder_log_var.0.weight: copying a param with shape torch.Size([20, 960]) from checkpoint, the shape in current model is torch.Size([20, 1024]).\n\tsize mismatch for decoder.0.0.weight: copying a param with shape torch.Size([960, 20]) from checkpoint, the shape in current model is torch.Size([1024, 20]).\n\tsize mismatch for decoder.0.0.bias: copying a param with shape torch.Size([960]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.1.0.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for decoder.1.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder.2.0.weight: copying a param with shape torch.Size([32, 32, 6, 6]) from checkpoint, the shape in current model is torch.Size([6144, 1024]).\n\tsize mismatch for decoder.2.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([6144]).\n\tsize mismatch for decoder.3.1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for decoder.3.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for decoder.4.0.weight: copying a param with shape torch.Size([32, 1, 6, 6]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for decoder.4.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "new_vae = VAE(latent_dim)\n",
    "new_vae.load_state_dict(trained_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "vital-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.9547e-01,  6.9035e-01,  4.2320e-01,  8.3899e-01,  1.8401e+00,\n",
      "         -3.1977e-01, -1.3820e+00,  1.0102e+00,  1.1069e+00,  2.2843e+00,\n",
      "         -1.0792e+00,  1.4654e-01, -5.1629e-01, -2.0900e-01, -1.2662e+00,\n",
      "         -7.9639e-01, -9.2992e-01, -2.1113e+00, -1.1167e+00,  2.3875e-01],\n",
      "        [-1.0631e-01, -5.8683e-01, -7.7416e-01,  2.0739e-01,  1.0154e+00,\n",
      "         -2.5371e-01,  6.2102e-01,  5.2228e-01,  1.5081e+00,  3.6975e-01,\n",
      "         -7.0279e-01, -3.5281e-02, -2.0121e+00,  1.0677e+00, -7.4286e-01,\n",
      "         -2.0439e-01,  4.1116e-01, -1.2027e+00,  5.0998e-01, -6.1429e-01],\n",
      "        [ 1.2173e+00,  4.7351e-01,  6.5448e-01,  1.4108e-01,  9.5603e-01,\n",
      "          1.1715e-01, -7.4444e-01, -1.1165e-01, -1.9058e+00,  7.7139e-01,\n",
      "         -5.7533e-01, -1.7457e+00,  4.3733e-01, -2.4241e-01, -1.5167e+00,\n",
      "          2.7018e-01, -8.8540e-01, -4.1748e-01,  4.9366e-01,  2.0629e+00],\n",
      "        [ 5.3454e-01, -1.4806e-01,  1.5921e+00, -1.1720e+00, -1.2667e-01,\n",
      "          1.1965e-01, -2.0763e-01, -2.7748e-01, -7.6237e-01, -1.5866e-01,\n",
      "         -3.1277e-04,  1.6551e+00,  7.7075e-01,  8.1962e-01, -8.9445e-01,\n",
      "          5.0369e-01, -1.2182e-01,  3.2998e-01,  2.4558e+00,  8.1138e-01],\n",
      "        [ 3.9315e-01,  1.7007e-01,  8.9453e-01, -1.5914e+00, -1.2343e+00,\n",
      "         -1.4056e+00,  1.2558e+00, -1.3856e+00, -8.0991e-01,  1.4315e+00,\n",
      "          3.6407e-01, -9.6960e-01, -6.6091e-01, -1.1942e+00,  6.7876e-01,\n",
      "          3.0391e-01, -5.8420e-01,  2.1247e-02,  1.0857e-01, -8.3392e-01],\n",
      "        [-1.1885e+00, -3.2982e-01, -1.3850e+00,  1.6075e+00,  4.2018e-01,\n",
      "         -1.1774e-01,  6.7166e-01, -2.9830e-01,  1.8238e+00, -1.4195e+00,\n",
      "         -9.6742e-02,  1.8703e-02,  1.5760e-02, -1.8232e-01, -2.4758e-01,\n",
      "          2.5958e+00, -3.4868e-01,  8.2751e-01,  1.9106e-01,  4.8787e-01]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument #2 'mat1' is on CPU, but expected it to be on GPU (while checking arguments for addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-01f16b416a87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torch.manual_seed(50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# ori_sample = mnist_train[4][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.figure()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plt.imshow(ori_sample.reshape((24,256)).detach().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3bc5e516b651>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[0;34m(self, n_sample)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3bc5e516b651>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reshape the tensor to be expected dimension for ConvTranspose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor for argument #2 'mat1' is on CPU, but expected it to be on GPU (while checking arguments for addmm)"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(50)\n",
    "samples = new_vae.generate_sample(6).detach().numpy()\n",
    "# ori_sample = mnist_train[4][0]\n",
    "# plt.figure()\n",
    "# plt.imshow(ori_sample.reshape((24,256)).detach().numpy())\n",
    "# samples, _, _ = new_vae(ori_sample.reshape((1,1,24,256)))\n",
    "# samples = samples.detach().numpy()\n",
    "for s in range(samples.shape[0]):\n",
    "    sample = samples[s,0,:,0:24].reshape(24,24)\n",
    "    plt.figure()\n",
    "    plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "exposed-novel",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.7327e-01,  3.9402e-01, -6.2465e-01,  2.2191e-01,  1.5748e+00,\n",
      "          8.0359e-01, -3.8358e-01, -1.1939e+00,  1.6041e-01,  8.0763e-01,\n",
      "         -5.2352e-01,  4.2479e-01, -2.4660e+00,  1.0039e-01, -1.0736e+00,\n",
      "         -8.4946e-02, -2.8789e-01, -6.3067e-01,  1.0571e-01, -6.5155e-01],\n",
      "        [ 9.0570e-01,  4.0763e-01,  2.6847e-01, -1.8115e+00, -1.3229e+00,\n",
      "         -1.0190e+00,  6.2301e-01,  8.2672e-01,  4.1756e-02, -9.3596e-01,\n",
      "          3.0391e-01,  7.4677e-01,  6.2171e-01,  7.2820e-01,  9.3001e-02,\n",
      "         -2.6045e+00,  1.0015e+00,  4.7062e-01,  5.7917e-01,  1.2868e-02],\n",
      "        [-1.7190e+00,  7.2532e-01, -2.2010e-03, -4.9515e-01, -2.0029e-01,\n",
      "          1.2886e+00,  4.5963e-03,  1.1657e+00,  3.9207e-01,  7.3427e-01,\n",
      "          2.0963e+00, -3.6145e-01, -4.1840e-01,  1.0191e+00,  7.4579e-01,\n",
      "          1.0750e+00,  3.7316e-01,  5.6823e-01, -5.0684e-01, -4.6419e-01],\n",
      "        [ 9.9191e-01, -4.9232e-02,  6.0604e-01, -1.8403e+00,  5.2091e-03,\n",
      "         -7.3371e-02, -2.3886e+00,  1.9127e-01, -4.4107e-01,  6.8373e-01,\n",
      "         -1.1816e+00, -2.7829e+00,  1.4432e+00, -8.1209e-01,  5.6911e-01,\n",
      "         -3.4218e-01, -4.5416e-01,  7.1023e-01,  1.4739e+00,  3.0234e+00]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument #2 'mat1' is on CPU, but expected it to be on GPU (while checking arguments for addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-3e1dd619abdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_raw_EEG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3bc5e516b651>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[0;34m(self, n_sample)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3bc5e516b651>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reshape the tensor to be expected dimension for ConvTranspose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor for argument #2 'mat1' is on CPU, but expected it to be on GPU (while checking arguments for addmm)"
     ]
    }
   ],
   "source": [
    "samples = new_vae.generate_sample(4).detach().numpy()\n",
    "plot_raw_EEG(samples, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-saying",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VAE.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
