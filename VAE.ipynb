{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow --quiet\n",
        "!pip install webdataset==0.1.62\n",
        "!pip install pytorch-model-summary"
      ],
      "metadata": {
        "id": "NB7Va37UmkNm",
        "outputId": "4ccf8a6d-a603-4c6b-aadd-41374ccdfbd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NB7Va37UmkNm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: webdataset==0.1.62 in /usr/local/lib/python3.7/dist-packages (0.1.62)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from webdataset==0.1.62) (1.21.6)\n",
            "Requirement already satisfied: braceexpand in /usr/local/lib/python3.7/dist-packages (from webdataset==0.1.62) (0.1.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-model-summary in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-model-summary) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-model-summary) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import csv\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# For visualize input\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import io\n",
        "import torchvision\n",
        "from torchvision import transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "Ln1f0NIUdLsL"
      },
      "id": "Ln1f0NIUdLsL",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "mathematical-beginning",
      "metadata": {
        "id": "mathematical-beginning"
      },
      "outputs": [],
      "source": [
        "import webdataset as wds\n",
        "from itertools import islice\n",
        "import struct, ast\n",
        "from importlib_metadata import version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "version('webdataset')"
      ],
      "metadata": {
        "id": "fMC2c7mLGIIQ",
        "outputId": "c6727503-e1a1-423d-c350-2a8681b6b10e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "id": "fMC2c7mLGIIQ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.1.62'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "framed-living",
      "metadata": {
        "id": "framed-living",
        "outputId": "bde5805e-b0f2-48e1-a7ed-80001858185e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "class Logger():\n",
        "    def __init__(self, mode='log'):\n",
        "        self.mode = mode\n",
        "        \n",
        "    def set_model_save_location(self, model_dir):\n",
        "        self.model_dir = f\"saved-model/{model_dir}\"\n",
        "        \n",
        "    def set_experiment(self, experiment_name):\n",
        "        self.experiment_name = experiment_name\n",
        "        log_format = '%(asctime)s %(message)s'\n",
        "        logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
        "                            format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "        fh = logging.FileHandler(os.path.join('training-logs', f'log-{experiment_name}-{datetime.datetime.today()}.txt'))\n",
        "        fh.setFormatter(logging.Formatter(log_format))\n",
        "        logging.getLogger().addHandler(fh)\n",
        "        self.writer = SummaryWriter(f\"runs/{experiment_name}\")\n",
        "            \n",
        "    def log(self, message=\"\"):\n",
        "        if self.mode == 'log':\n",
        "            logging.info(message)\n",
        "        elif self.mode == 'debug':\n",
        "            print(message)\n",
        "\n",
        "    def save_model(self, model, info):\n",
        "        torch.save(model.state_dict(), f\"{self.model_dir}/model-{self.experiment_name}-{info}\")\n",
        "        \n",
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)\n",
        "\n",
        "def add_chan_dim(x):\n",
        "    x = torch.tensor(x)\n",
        "#     x = torch.transpose(x, 0, 1)\n",
        "    return torch.unsqueeze(x,0)\n",
        "\n",
        "\n",
        "def selectLabel(x,lbl):\n",
        "    # # function to select desired label\n",
        "    lbl_idx = [\"id\",\"sex\",\"age\",\"handedness\",\"index\"].index(lbl.lower())\n",
        "    x = x.decode(\"utf-8\").split(\",\")\n",
        "    # return x if lbl_idx == 0 else float(x)\n",
        "    return 1\n",
        "    \n",
        "s3_url = 'https://childmind.s3.us-west-1.amazonaws.com/python/childmind_train.tar' # replace 'train' with 'val' and 'test' accordingly\n",
        "train_data = wds.WebDataset(s3_url).decode().map_dict(npy=add_chan_dim, cls=lambda x: selectLabel(x,'sex')).to_tuple(\"npy\",\"cls\")\n",
        "\n",
        "s3_url = 'https://childmind.s3.us-west-1.amazonaws.com/python/childmind_val.tar' # replace 'train' with 'val' and 'test' accordingly\n",
        "val_data = wds.WebDataset(s3_url).decode().map_dict(npy=add_chan_dim, cls=lambda x: selectLabel(x,'sex')).to_tuple(\"npy\",\"cls\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ignored-eagle",
      "metadata": {
        "id": "ignored-eagle"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module): \n",
        "    \n",
        "    def __init__(self, latent_dim):\n",
        "        # latent_dim: dimension of the latent representation vector\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.beta = 4\n",
        "        self.gamma = 1000.\n",
        "        self.loss_type = 'H'\n",
        "        self.C_max = torch.tensor([25])\n",
        "        self.C_stop_iter = 1e5\n",
        "\n",
        "        encoder_l = [self.encoder_conv_block(True)]\n",
        "        for i in range(2):\n",
        "            encoder_l.append(self.encoder_conv_block())\n",
        "        encoder_l.append(self.encoder_conv_block(False, 32, 32, 3, 1, 0))\n",
        "        encoder_l.append(nn.Flatten())\n",
        "        self.encoder_before_last = nn.ModuleList(encoder_l)\n",
        "        self.encoder_mu = self.encoder_linear_block(960, latent_dim)\n",
        "        self.encoder_log_var = self.encoder_linear_block(960, latent_dim)\n",
        "                            \n",
        "        decoder_l = [self.decoder_linear_block(latent_dim, 960)]\n",
        "        decoder_l.append(self.decoder_conv_block(False, 32, 32, 3, 1, 0))\n",
        "        for i in range(2):\n",
        "            decoder_l.append(self.decoder_conv_block())\n",
        "        decoder_l.append(self.decoder_conv_block(True))\n",
        "        self.decoder = nn.ModuleList(decoder_l)\n",
        "    \n",
        "    def encoder_conv_block(self, is_start=False, in_channels=32, out_channels=32, kernel_size=6, stride=2, padding=2):\n",
        "        if is_start:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(1, out_channels, kernel_size, stride, padding),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU()\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU()\n",
        "            )\n",
        "    def encoder_linear_block(self, in_chan, out_chan):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_chan, out_chan)\n",
        "        )\n",
        "    \n",
        "    def decoder_conv_block(self, is_last=False, in_channels=32, out_channels=32, kernel_size=6, stride=2, padding=2):\n",
        "        if is_last:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, 1, kernel_size, stride, padding),\n",
        "                nn.BatchNorm2d(1),\n",
        "                nn.LeakyReLU()\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU()\n",
        "            )\n",
        "    \n",
        "    def decoder_linear_block(self, in_chan, out_chan):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_chan, out_chan),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for f in self.encoder_before_last:\n",
        "            x = f(x)\n",
        "\n",
        "        mu = self.encoder_mu(x)\n",
        "        log_var = self.encoder_log_var(x)\n",
        "\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        \n",
        "        x = self.decoder[0](z)\n",
        "        x = x.view(-1, 32, 1, 30) # reshape the tensor to be expected dimension for ConvTranspose\n",
        "        for i in range(1,len(self.decoder)):\n",
        "            f = self.decoder[i]\n",
        "            x = f(x) \n",
        "            \n",
        "        return x, mu, log_var\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5*log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling\n",
        "        return sample\n",
        "\n",
        "    def sample_z(self, mu, sigma):\n",
        "        # Input\n",
        "        #     mu:     [batch_size, self.latent_size] the predicted mu value for each sample in the batch\n",
        "        #     sigma:  [batch_size, self.latent_size] the predicted diag elem of sigma value for each sample in the batch\n",
        "        # Output\n",
        "        #     z: [batch_size, self.latent_size] the latent representation of each sample in the batch\n",
        "        # Reference: https://agustinus.kristia.de/techblog/2016/12/10/variational-autoencoder/\n",
        "        \n",
        "        # eps ~ N(0,1)        \n",
        "        batch_size = mu.size()[0]\n",
        "        eps = torch.randn((batch_size,1), device=device, dtype=dtype)\n",
        "        z = mu + sigma/2*eps\n",
        "        \n",
        "        return z\n",
        "\n",
        "    def loss_function(self,\n",
        "                      *args,\n",
        "                      **kwargs) -> dict:\n",
        "        recons = args[0]\n",
        "        input = args[1]\n",
        "        mu = args[2]\n",
        "        log_var = args[3]\n",
        "        kld_weight = kwargs['M_N']  # Account for the minibatch samples from the dataset\n",
        "\n",
        "        recons_loss =F.mse_loss(recons, input)\n",
        "\n",
        "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
        "\n",
        "        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl\n",
        "            loss = recons_loss + self.beta * kld_weight * kld_loss\n",
        "        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf\n",
        "            self.C_max = self.C_max.to(input.device)\n",
        "            C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n",
        "            loss = recons_loss + self.gamma * kld_weight* (kld_loss - C).abs()\n",
        "        else:\n",
        "            raise ValueError('Undefined loss type.')\n",
        "\n",
        "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c39cb173",
      "metadata": {
        "scrolled": false,
        "id": "c39cb173",
        "outputId": "e4f64788-2b81-4cbb-ff8d-174f96633bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n",
            "         Layer (type)         Output Shape         Param #     Tr. Param #\n",
            "===========================================================================\n",
            "             Conv2d-1     [1, 32, 12, 128]           1,184           1,184\n",
            "        BatchNorm2d-2     [1, 32, 12, 128]              64              64\n",
            "          LeakyReLU-3     [1, 32, 12, 128]               0               0\n",
            "             Conv2d-4       [1, 32, 6, 64]          36,896          36,896\n",
            "        BatchNorm2d-5       [1, 32, 6, 64]              64              64\n",
            "          LeakyReLU-6       [1, 32, 6, 64]               0               0\n",
            "             Conv2d-7       [1, 32, 3, 32]          36,896          36,896\n",
            "        BatchNorm2d-8       [1, 32, 3, 32]              64              64\n",
            "          LeakyReLU-9       [1, 32, 3, 32]               0               0\n",
            "            Conv2d-10       [1, 32, 1, 30]           9,248           9,248\n",
            "       BatchNorm2d-11       [1, 32, 1, 30]              64              64\n",
            "         LeakyReLU-12       [1, 32, 1, 30]               0               0\n",
            "           Flatten-13             [1, 960]               0               0\n",
            "            Linear-14              [1, 10]           9,610           9,610\n",
            "            Linear-15              [1, 10]           9,610           9,610\n",
            "            Linear-16             [1, 960]          10,560          10,560\n",
            "              ReLU-17             [1, 960]               0               0\n",
            "   ConvTranspose2d-18       [1, 32, 3, 32]           9,248           9,248\n",
            "       BatchNorm2d-19       [1, 32, 3, 32]              64              64\n",
            "         LeakyReLU-20       [1, 32, 3, 32]               0               0\n",
            "   ConvTranspose2d-21       [1, 32, 6, 64]          36,896          36,896\n",
            "       BatchNorm2d-22       [1, 32, 6, 64]              64              64\n",
            "         LeakyReLU-23       [1, 32, 6, 64]               0               0\n",
            "   ConvTranspose2d-24     [1, 32, 12, 128]          36,896          36,896\n",
            "       BatchNorm2d-25     [1, 32, 12, 128]              64              64\n",
            "         LeakyReLU-26     [1, 32, 12, 128]               0               0\n",
            "   ConvTranspose2d-27      [1, 1, 24, 256]           1,153           1,153\n",
            "       BatchNorm2d-28      [1, 1, 24, 256]               2               2\n",
            "         LeakyReLU-29      [1, 1, 24, 256]               0               0\n",
            "===========================================================================\n",
            "Total params: 198,647\n",
            "Trainable params: 198,647\n",
            "Non-trainable params: 0\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class ToCorrectSizeTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        correct_size_sample = torch.zeros(1,24,256)\n",
        "        correct_size_sample[0,:,0:28] = sample[0,2:26,:]\n",
        "        return correct_size_sample\n",
        "\n",
        "vae = VAE(10)\n",
        "vae = vae.to(device=device)\n",
        "# mnist_train = torchvision.datasets.MNIST('./mnist', train=True, download=True, transform = transforms.Compose([transforms.ToTensor(),ToCorrectSizeTensor()]))\n",
        "# mnist_test = torchvision.datasets.MNIST('./mnist', train=False, download=True, transform = transforms.Compose([transforms.ToTensor(), ToCorrectSizeTensor()]))\n",
        "# sample = torch.unsqueeze(mnist_train[0][0],0)\n",
        "# sample = sample.to(device=device)\n",
        "\n",
        "from pytorch_model_summary import summary\n",
        "# print(summary(vae, sample, show_input=False)) \n",
        "print(summary(vae, torch.zeros((1, 1, 24, 256), device=device), show_input=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d6724f9",
      "metadata": {
        "id": "9d6724f9"
      },
      "source": [
        "KL divergence loss\n",
        "![kl_loss](https://github.com/dungscout96/deep-representation-learning-EEG/blob/master/images/kl_loss.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "48aa32b1",
      "metadata": {
        "id": "48aa32b1"
      },
      "outputs": [],
      "source": [
        "# def beta_vae_loss(mu_hat, sigma_hat, x_hat, x_target):\n",
        "#     likelihood_loss = F.mse_loss(x_hat, x_target)\n",
        "#     kl_loss = 0.5 * torch.sum(torch.exp(sigma_hat) + torch.pow(mu_hat,2) - torch.ones((1,mu_hat.size()[1]), device=device, dtype=dtype) - mu_hat, axis=1)\n",
        "#     return torch.mean(likelihood_loss + kl_loss) # average the loss of batch\n",
        "\n",
        "def train(model, loader_train, optimizer, loader_val, epochs, logger, device, dtype):\n",
        "    \"\"\" \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    - logger: Logger object for logging purpose\n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    loss_array = []\n",
        "    num_batch = 0\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    print('Begin trainning...')\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            if e == 0:\n",
        "                num_batch += 1\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            # mu_hat, sigma_hat, x_hat = model(x)\n",
        "            \n",
        "            # loss = beta_vae_loss(mu_hat, sigma_hat, x_hat, x)\n",
        "            \n",
        "            x_hat, mu_hat, sigma_hat = model(x)\n",
        "            \n",
        "            loss_dict = model.loss_function(x_hat, x, mu_hat, sigma_hat,M_N=1)\n",
        "            loss = loss_dict[\"loss\"]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_array.append(loss.item())\n",
        "            if t % 100 == 0:\n",
        "                # logger.writer.add_scalar(\"Loss/train\", loss.item(), e*num_batch+t)\n",
        "                # logger.log('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "                print(f'Epoch {e}, Iteration {t}, loss = {loss.item()}')\n",
        "#         train_acc = check_accuracy(loader_train, 'train', model, device, dtype, logger)\n",
        "#         logger.writer.add_scalar(\"Acc/train\", train_acc, e)        \n",
        "        # get validation loss\n",
        "#         model.eval()\n",
        "#         val_loss = check_accuracy(loader_val, 'val', model, device, dtype, logger)\n",
        "#         logger.writer.add_scalar(\"Acc/valid\", val_acc, e)        \n",
        "#         logger.log()\n",
        "        \n",
        "        # Save model per fixed epoch interval\n",
        "        # if e > 0 and e % 10 == 0:\n",
        "        #     logger.save_model(model,f\"epoch{e}\")\n",
        "#         elif val_acc >= 0.83:\n",
        "#             logger.save_model(model,f\"valacc83-epoch{e}\")\n",
        "#         elif val_acc >= 0.84:\n",
        "#             logger.save_model(model,f\"valacc84-epoch{e}\")\n",
        "    # save final model\n",
        "    # logger.save_model(model,f\"epoch{e}\")\n",
        "    return model, loss_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "54ec2618",
      "metadata": {
        "id": "54ec2618",
        "outputId": "c197bb05-d769-4c4f-a52b-5e803ecb9c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin trainning...\n",
            "Epoch 0, Iteration 0, loss = 2235.8779296875\n",
            "Epoch 0, Iteration 100, loss = 472.8282165527344\n",
            "Epoch 0, Iteration 200, loss = 3559.554931640625\n",
            "Epoch 1, Iteration 0, loss = 2234.187255859375\n",
            "Epoch 1, Iteration 100, loss = 472.5002136230469\n",
            "Epoch 1, Iteration 200, loss = 3559.206298828125\n",
            "Epoch 2, Iteration 0, loss = 2234.0\n",
            "Epoch 2, Iteration 100, loss = 472.40863037109375\n",
            "Epoch 2, Iteration 200, loss = 3559.112548828125\n",
            "Epoch 3, Iteration 0, loss = 2233.8232421875\n",
            "Epoch 3, Iteration 100, loss = 472.2935485839844\n",
            "Epoch 3, Iteration 200, loss = 3558.920654296875\n",
            "Epoch 4, Iteration 0, loss = 2233.4970703125\n",
            "Epoch 4, Iteration 100, loss = 472.2217712402344\n",
            "Epoch 4, Iteration 200, loss = 3558.84326171875\n",
            "Epoch 5, Iteration 0, loss = 2233.56396484375\n",
            "Epoch 5, Iteration 100, loss = 472.1435241699219\n",
            "Epoch 5, Iteration 200, loss = 3558.7275390625\n",
            "Epoch 6, Iteration 0, loss = 2233.45556640625\n",
            "Epoch 6, Iteration 100, loss = 472.1318359375\n",
            "Epoch 6, Iteration 200, loss = 3558.728515625\n",
            "Epoch 7, Iteration 0, loss = 2233.279296875\n",
            "Epoch 7, Iteration 100, loss = 472.0150451660156\n",
            "Epoch 7, Iteration 200, loss = 3558.727783203125\n",
            "Epoch 8, Iteration 0, loss = 2232.684814453125\n",
            "Epoch 8, Iteration 100, loss = 472.2129821777344\n",
            "Epoch 8, Iteration 200, loss = 3559.46923828125\n",
            "Epoch 9, Iteration 0, loss = 2233.16650390625\n",
            "Epoch 9, Iteration 100, loss = 472.3148193359375\n",
            "Epoch 9, Iteration 200, loss = 3558.699951171875\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b289cf46-6e7d-4232-a363-aeaacd86122e\", \"vae_10_epochs\", 816683)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fb434610-064a-4469-9b7c-3fe067541821\", \"losses.npy\", 22368)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "vae = VAE(10)\n",
        "logger = Logger()\n",
        "logger.set_model_save_location('VAE')\n",
        "seed = 0\n",
        "experiment = f'VAE-seed{seed}'\n",
        "# logger.set_experiment(experiment)\n",
        "\n",
        "# use mnist to test\n",
        "\n",
        "batch_size = 256\n",
        "loader_train = DataLoader(train_data, batch_size=batch_size)\n",
        "loader_val = DataLoader(val_data, batch_size=batch_size)\n",
        "# loader_train = DataLoader(mnist_train, batch_size=batch_size)\n",
        "# loader_val = DataLoader(mnist_test, batch_size=batch_size)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=0.00001)\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "trained_model, loss_values = train(vae, loader_train, optimizer, loader_val, 10, logger, device, dtype)\n",
        "\n",
        "from google.colab import files\n",
        "np.save('losses.npy',loss_values)\n",
        "torch.save(trained_model.state_dict(),'vae_10_epochs')\n",
        "files.download('vae_10_epochs') \n",
        "files.download('losses.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5fd5d057",
      "metadata": {
        "id": "5fd5d057",
        "outputId": "9bcd5906-e271-4522-a149-a727946730f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32.0\n",
            "30.0\n"
          ]
        }
      ],
      "source": [
        "def out_W(W, F, P, S):\n",
        "    return np.floor((W - F + 2*P)/S + 1)\n",
        "\n",
        "W = 256\n",
        "for i in range(3):\n",
        "    W = out_W(W, 6, 2, 2)\n",
        "\n",
        "print(W)\n",
        "print(out_W(W, 3, 0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "09c5e2bd",
      "metadata": {
        "id": "09c5e2bd",
        "outputId": "d7724e26-8cfb-446b-d6c5-9bb309344683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f039fe88cd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rRxXAQF60PNB"
      },
      "id": "rRxXAQF60PNB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "VAE.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}